## How to Reproduce the Experiment ##
To reproduce the experiments:

1. **Follow the [installation instructions](#-required-installation) below** to set up the environment. 
2. **Run the three configurations** (`base_model`, `reactagent`,`without_dsl`(proposal)) for all selected local models.
   > *Note:* In this version, switching between models must be done **manually in the code**.
3. For commercial models, the experiments were conducted using their official web interfaces:
   - [DeepSeek](https://chat.deepseek.com/)
   - [Claude](https://claude.ai/)
   - [Gemini](https://gemini.google.com/app)


## Project Directory Structure

- `/config`  
  Contains the configuration files required for project execution.

- `/data`  
  Includes PDF documents that serve as the knowledge base to be queried by the models.

- `/dataset`  
  Dataset used for evaluation:
  - `/answers`: Contains the expected answers to the questions.
  - `/questions`: Contains the set of questions used in the evaluation.
    - `/prompts`: Contains prompts to extract structured answers from textual responses, facilitating subsequent automated comparison.

- `/logs`  
  Stores the system execution logs.

- `/reports`  
  Contains the generated reports in Excel and CSV formats, summarizing the evaluation results.

- `/results`  
  Saves the answers generated by the models after the evaluation process.

- `/src`  
  Main directory for the project's source code.

- `.env`
  You must set `PYTHONPATH=src` in this file so that subsequent commands run correctly, as this allows Python to recognize the source code directory as part of the module search path.

## ðŸ›  Required Installation

To set up the project, make sure to follow these steps:

### 1. Install Ollama locally
- Allows you to run language models on your machine without external connection.
- Check the official documentation for your operating system: https://ollama.com

### 2. Install Python
- Ensure Python is installed (recommended: version 3.10 or higher).
- You can verify this with:
  ```bash
  python --version
  ```

### 3. Install project dependencies
- From the project root, run in the terminal:
  ```bash
  pip install -r requirements.txt
  ```

## Running the Evaluation Procedure
There are 3 different configurations to evaluate the models. Before running the command, you need to set the model to evaluate in each corresponding file and ensure that the /reports and /results folders exist.

1. Use the default configuration
```bash
python src\main_base_model_evaluation.py
```

2. Use the Llamaindex agent
```bash
python src\main_reactagent_evaluation.py
```

3. Use the proposed approach
- With DSL
```bash
python src\main_workflow.py
```
- Without DSL
```bash
python src\main_workflow_without_dsl.py
```

## Running the agent with Streamlit interface
  ```bash
  streamlit run src\main_chatbot_without_dsl.py
  ```

## Running tests with `pytest`

From the project root directory, run the following command in the terminal:

```bash
pytest
```
You need to have the `pytest` and `pytest-cov` packages installed.

## Notes on the behavior of the workflow built by the DSL (Outdated)

### Global Context
- The query documents are already available in the global context.
- They can be used directly in prompts using the `{}` syntax.

### Step Execution (`steps`)
- Each step generates a result stored in `output`.
- In composite steps, only the output of the last step is saved.
- If a step produces no result (e.g., an `if` condition that is not met), it returns `None`.
- The main class returns the previous result if it exists; otherwise, it returns "no output".

### Applying Filters (`apply filters`)
- Returns a tuple with two elements:
  1. Documents that meet the filters (`filtered docs`).
  2. Values that do not match (`unmatched values`).

### Step Type: `set variable`
- The `source` field defines the value assigned to the variable.
- It can be a Python expression and can be extracted from the context.

### Limitations and Considerations
- This implementation is based on patterns observed in the use case.
- **Security aspects have not been considered.**
- Only JSON outputs are accepted:
  - A single level of nesting.
  - Only objects (no lists).
- Metadata is excluded from processing.

### Memory (`add_to_memory`)
- Only the `description` field is formatted.
- It can include variables using the `{variable}` syntax.

### Evaluation (`evaluation`)
- In the current DSL, if the evaluation always determines that an answer is "fabricated," it may cause an infinite loop.
- This behavior must be controlled to avoid execution errors.

## Estructura del Directorio del Proyecto

- `/config`  
  Contiene los archivos de configuraci贸n necesarios para la ejecuci贸n del proyecto.

- `/data`  
  Incluye los documentos en formato PDF que sirven como base de conocimiento a consultar por los modelos.

- `/dataset`  
  Conjunto de datos utilizado para la evaluaci贸n:
  - `/answers`: Contiene las respuestas esperadas respecto a las preguntas.
  - `/questions`: Contiene el conjunto de preguntas utilizadas en la evaluaci贸n.
    - `/prompts`: Cotiene prompts para extraer respuestas estructuradas a partir de respuestas textuales, facilitando la comparaci贸n automatizada posterior

- `/logs`  
  Almacena los registros de ejecuci贸n del sistema.

- `/reports`  
  Contiene los informes generados en formatos Excel y CSV, que resumen los resultados de la evaluaci贸n.

- `/results`  
  Guarda las respuestas generadas por los modelos tras el proceso de evaluaci贸n.

- `/src`  
  Directorio principal del c贸digo fuente del proyecto.

- `.env`
  Es necesario establecer `PYTHONPATH=src` en este fichero para que los comandos posteriores se ejecuten correctamente, ya que esto permite que Python reconozca el directorio de c贸digo fuente como parte del entorno de b煤squeda de m贸dulos.

##  Instalaci贸n necesaria

Para poner en marcha el proyecto, aseg煤rate de seguir estos pasos:

### 1. Instalar Ollama localmente
- Permite ejecutar modelos de lenguaje en tu m谩quina sin necesidad de conexi贸n externa.
- Consulta la documentaci贸n oficial para tu sistema operativo: https://ollama.com

### 2. Instalar Python
- Aseg煤rate de tener Python instalado (recomendado: versi贸n 3.10 o superior).
- Puedes verificarlo con:
  ```bash
  python --version
  ```

### 3. Instalar dependencias del proyecto
- Desde la ra铆z del proyecto, ejecuta en la terminal:
  ```bash
  pip install -r requirements.txt
  ```

## Ejecuci贸n del procedimiento de evaluaci贸n
Existe 3 configuraciones distintas para evaluar los modelos. Antes de la ejecuci贸n del comando, es necesario establecer el modelo a evaluar dentro de cada fichero correspondiente. 

1. Usa la configuraci贸n por defecto
```bash
python main_base_model_evaluation.py
```

2. Usa el agente de Llamaindex
```bash
python main_reactagent_evaluation.py
```

3. Usa el enfoque propuesto
- Con DSL
```bash
python main_workflow.py
```
- Sin DSL
```bash
python main_workflow_without_dsl.py
```

## Ejecuci贸n de pruebas con `pytest`

Desde el directorio ra铆z del proyecto, ejecuta el siguiente comando en la terminal:

```bash
pytest
```
Se necesitan tener instalados los paquetes pytest y pytest-cov.

## Notas sobre el comportamiento del workflow construido por el DSL (Desactualizado)

### Contexto Global
- Los documentos de consulta (`query documents`) ya est谩n disponibles en el contexto global.
- Pueden utilizarse directamente en los prompts mediante la sintaxis `{}`.

### Ejecuci贸n de Pasos (`steps`)
- Cada paso genera un resultado que se almacena en `output`.
- En pasos compuestos (`composite`), solo se guarda la salida del 煤ltimo paso.
- Si un paso no produce resultado (por ejemplo, en una condici贸n `if` que no se cumple), se devuelve `None`.
- La clase principal se encarga de devolver el resultado anterior si existe; en caso contrario, retorna `"no output"`.

### Aplicaci贸n de Filtros (`apply filters`)
- Devuelve una tupla con dos elementos:
  1. Documentos que cumplen con los filtros (`filtered docs`).
  2. Valores que no coinciden (`unmatched values`).

### Tipo de Paso: `set variable`
- El campo `source` define el valor que se asignar谩 a la variable.
- Puede ser una expresi贸n en Python y puede extraerse del contexto.

### Limitaciones y Consideraciones
- Esta implementaci贸n se basa en patrones observados en el caso de uso.
- **No se han considerado aspectos de seguridad.**
- Solo se aceptan salidas en formato JSON:
  - Un 煤nico nivel de anidamiento.
  - Solo objetos (no listas).
- Los metadatos est谩n excluidos del procesamiento.

### Memoria (`add_to_memory`)
- Solo el campo `description` se formatea.
- Puede incluir variables mediante la sintaxis `{variable}`.

### Evaluaci贸n (`evaluation`)
- En el DSL actual, si la evaluaci贸n siempre determina que una respuesta es "inventada", puede producir un bucle infinito.
- Este comportamiento debe ser controlado para evitar errores en la ejecuci贸n.

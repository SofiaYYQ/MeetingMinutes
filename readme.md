## Estructura del Directorio del Proyecto

- `/config`  
  Contiene los archivos de configuraci贸n necesarios para la ejecuci贸n del proyecto.

- `/data`  
  Incluye los documentos en formato PDF que sirven como base de conocimiento a consultar por los modelos.

- `/dataset`  
  Conjunto de datos utilizado para la evaluaci贸n:
  - `/answers`: Contiene las respuestas esperadas respecto a las preguntas.
  - `/questions`: Contiene el conjunto de preguntas utilizadas en la evaluaci贸n.
    - `/prompts`: Cotiene prompts para extraer respuestas estructuradas a partir de respuestas textuales, facilitando la comparaci贸n automatizada posterior

- `/logs`  
  Almacena los registros de ejecuci贸n del sistema.

- `/reports`  
  Contiene los informes generados en formatos Excel y CSV, que resumen los resultados de la evaluaci贸n.

- `/results`  
  Guarda las respuestas generadas por los modelos tras el proceso de evaluaci贸n.

- `/src`  
  Directorio principal del c贸digo fuente del proyecto.

- `.env`
  Es necesario establecer `PYTHONPATH=src` en este fichero para que los comandos posteriores se ejecuten correctamente, ya que esto permite que Python reconozca el directorio de c贸digo fuente como parte del entorno de b煤squeda de m贸dulos.

##  Instalaci贸n necesaria

Para poner en marcha el proyecto, aseg煤rate de seguir estos pasos:

### 1. Instalar Ollama localmente
- Permite ejecutar modelos de lenguaje en tu m谩quina sin necesidad de conexi贸n externa.
- Consulta la documentaci贸n oficial para tu sistema operativo: https://ollama.com

### 2. Instalar Python
- Aseg煤rate de tener Python instalado (recomendado: versi贸n 3.10 o superior).
- Puedes verificarlo con:
  ```bash
  python --version
  ```

### 3. Instalar dependencias del proyecto
- Desde la ra铆z del proyecto, ejecuta en la terminal:
  ```bash
  pip install -r requirements.txt
  ```

## Ejecuci贸n del procedimiento de evaluaci贸n
Existe 3 configuraciones distintas para evaluar los modelos. Antes de la ejecuci贸n del comando, es necesario establecer el modelo a evaluar dentro de cada fichero correspondiente. 

1. Usa la configuraci贸n por defecto
```bash
python src\main_base_model_evaluation.py
```

2. Usa el agente de Llamaindex
```bash
python src\main_reactagent_evaluation.py
```

3. Usa el enfoque propuesto
- Con DSL
```bash
python src\main_workflow.py
```
- Sin DSL
```bash
python src\main_workflow_without_dsl.py
```

## Ejecuci贸n del agente con interfaz streamlit
  ```bash
  streamlit run src\main_chatbot_without_dsl.py
  ```

## Ejecuci贸n de pruebas con `pytest`

Desde el directorio ra铆z del proyecto, ejecuta el siguiente comando en la terminal:

```bash
pytest
```
Se necesitan tener instalados los paquetes pytest y pytest-cov.
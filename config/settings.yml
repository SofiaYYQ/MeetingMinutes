app:
  log:
    log_level: "INFO"
  general:
    # "chat", "evaluate", "normal" 
    execute_mode: "evaluate"
    llm:
        # Puede no ser necesario, si no se indica el base_url, pues no se usa un server
        # use_server: true 
        # base_url: "http://156.35.95.18:11434"
        
        model_name: "qwen3:4b"
        # embedding_model_name: "snowflake-arctic-embed2"
        request_timeout: 600.0
        # system_prompt: "Responde siempre en español"
        temperature: 0.3
    
  evaluation_config:
    questions_file_path: "dataset/questions/1-questions_persons.txt"
    prompts_file_path: "dataset/questions/prompts/1-questions_persons_prompts.txt"
    answers_file_path: "dataset/answers/1-questions_persons_answers.txt"
    results_folder_path: "results"
    reports_folder_path: "reports"
    
  data_processing:
    data_folder_path: "data"
    metadata_config:
      data_description: "Las actas de las reuniones de la comunidad de vecinos incluyen detalles sobre la fecha, la hora de inicio y fin, y el lugar de celebración. Además, se enumeran los asistentes y se presenta el orden del día."
      fields_info:
        - name: "fecha"
          type: "str"
          description: "Es la fecha en la que se celebró la reunión. Está en formato DD/MM/AAAA."
        - name: "num_asistentes"
          type: "int"
          description: "Es el número de asistentes a la reunión"
        - name: "lista_asistentes"
          type: "list"
          description: "Es la lista de nombres de los asistentes de la reunión."
        - name: "presidente"
          type: "str"
          description: "Es el nombre del presidente o presidenta de la reunión."
        - name: "secretario"
          type: "str"
          description: "Es el nombre del secretorio o secretaria de la reunión."
    # chunks_config:
    #   chunk_size: 512
    #   chunk_overlap: 0

  workflow:
    - step_type: "composite"
      id: "filter_documents"
      output: "filtered_documents"
      steps:
        - step_type: "llm_call"
          id: "get_person_name_from_query"
          prompt: |
            Extrae el nombre completo (nombre y apellidos) de la persona mencionada en la siguiente consulta. Si no se menciona ninguna persona, devuelve "None".

            Devuelve el resultado en formato JSON con la clave "persona", siguiendo exactamente esta estructura:

            {{"lista_asistentes": "nombre y apellidos de la persona"}}
              o
            {{"lista_asistentes": "None"}}

            CONSULTA: {query}
          json_output: True
          output: name_obj

        - step_type: "llm_call"
          id: "get_date_from_query"
          prompt: |
            Extrae la fecha mencionada en la siguiente consulta en formato DD/MM/AAAA. Si no se menciona ninguna fecha, devuelve "None".

            Si la fecha está incompleta, utiliza "%" para el día, el mes o el año faltante. Por ejemplo:
            - Si solo se menciona "junio de 2023", devuelve: {{"fecha": "%/06/2023"}}
            - Si solo se menciona "2022", devuelve: {{"fecha": "%/%/2022"}}

            Devuelve el resultado en formato JSON con la clave "fecha", siguiendo exactamente una de estas dos estructuras:

            {{"fecha": "DD/MM/AAAA"}}
            o
            {{"fecha": "None"}}

            CONSULTA: {query}
          json_output: True
          output: date_obj
        - step_type: "action"
          id: "apply_filters"
          action: "apply_filters"
          inputs: ["documents", "name_obj", "date_obj"] # documentos y filtros
          output: apply_filters_results
        - step_type: "if"
          id: "check_number_valid_filters"
          condition: len(apply_filters_results[2]) == 0
          output: "no_output"
          if_true:
            step_type: "action"
            id: "add_to_memory_documents_filtering_no_filters"
            action: "add_to_memory"
            name: "Filtrado de documentos"
            description: "Se identifican y excluyen los documentos que no cumplen con los criterios establecidos en la consulta."
            result: "A partir de la consulta '{query}', no se han extraído ningún filtro. Por lo tanto no se excluyen ningún documento."
            output: "no_output" # quitar para que no sea obligatorio
          if_false:
            step_type: "if"
            id: "check_number_filtered_documents"
            condition: "len(apply_filters_results[0]) == 0"
            output: "no_output" # quitar para que no sea obligatorio
            if_true:
              step_type: "action"
              id: "add_to_memory_documents_filtering_with_filtered_documents"
              action: "add_to_memory"
              output: "no_output" # quitar para que no sea obligatorio
              name: "Filtrado de documentos"
              description: "Se identifican y excluyen los documentos que no cumplen con los criterios establecidos en la consulta."
              result: | 
                A partir de la consulta '{query}', se han identificado los siguientes filtros aplicables: {apply_filters_results[2]}.\n
                Tras aplicar estos filtros, se ha determinado que ninguno de los documentos disponibles cumplen los criterios.\n
                Estos son los VALROES que no se mencionan en ningún documento: [{apply_filters_results[1]}].\n
                Esto puede deberse a varias razones:\n
                - Si no se menciona el nombre, es posible que la persona no figure en los registros o que su nombre esté mal escrito.\n
                - Si no se menciona la fecha, es probable que no existan reuniones registradas en esa fecha específica.\n
                - Si no se menciona ni el nombre ni la fecha, probablemente que no existe ni la persona ni la reunión en esa fecha. \n
                En caso de que la lista de VALORES está vacía, significa que la persona y la reunión sí existen, pero la persona no asistió a la reunión de la fecha indicada.\n
                Por lo tanto, todos los documentos han sido descartados. Documentos excluidos: {apply_filters_results[3]}.
              
            if_false:
              step_type: "action"
              id: "add_to_memory_documents_filtering_all_documents"
              action: "add_to_memory"
              name: "Filtrado de documentos"
              description: "Se identifican y excluyen los documentos que no cumplen con los criterios establecidos en la consulta."
              result: |
                A partir de la consulta '{query}', se han identificado los siguientes filtros aplicables: {apply_filters_results[2]}. 
                Se han revisado todos los documentos disponibles y se han excluido aquellos que no coinciden con los filtros especificados.
                Los documentos que no cumplen con estos criterios son los siguientes: {apply_filters_results[3]}
              output: "no_output" # quitar para que no sea obligatorio

        - step_type: "set_variable" # igual se puede pasar a donde se creao
          id: "set_filtered_documents"
          source: "apply_filters_results[0]"
          output: "filtered_documents"

            
    - step_type: "composite"
      id: "check_query_type"
      output: "subquery" # corregir para que no hace falta
      steps:
        - step_type: "action"
          id: "check_comparative_query"
          action: "check_terms_in_text"
          inputs: [["mejor", "mejores", "peor", "peores", "superior", "superiores", "inferior", "inferiores",
            "mayor", "mayores", "menor", "menores", "más", "menos"], query]
          output: is_comparative_query
        - step_type: "action"
          id: "check_global_query"
          action: "check_terms_in_text"
          inputs: [["reuniones"], query]
          output: is_global_query
        - step_type: "if"
          id: "check_global_and_not_comparative_query"
          condition: "is_global_query and not is_comparative_query"
          output: "subquery" # corregir para que no hace falta
          if_true:
            step_type: "composite"
            id: "composite_transform_to_subquery"
            output: "subquery" # corregir para que no hace falta
            steps:
            - step_type: "llm_call"
              id: "transform_to_subquery" 
              prompt: |
                Transforma preguntas globales que requieren revisar múltiples documentos (como actas de reuniones) en preguntas individuales que puedan aplicarse a cada documento por separado.
                Pasos:
                1.	Identificar el sujeto y la acción de la oración:
                2.	Convierte la pregunta global en una individual:
                ¿[Acción] [Sujeto] [Por el complemento que se pregunta si existe, sino este documento]?
                Ejemplo
                Pregunta global: ¿En cuántos informes médicos se menciona a Laura Sánchez?
                Pregunta individual: ¿Es mencionada Laura Sánchez en este informe médico?

                Ejemplo
                Pregunta global: ¿Cuántas veces aparece el nombre de Javier Torres?
                Pregunta individual: ¿Aparece el nombre de Javier Torres en este documento?
                            
                Transforma ahora la siguiente pregunta global en una pregunta individual aplicable a un solo documento. 
                Pregunta global: {query}

                Responde en formato json con dos claves "pregunta_global", "pregunta_individual":
                {{
                    "pregunta_global":"pregunta original",
                    "pregunta_individual":"pregunta transformada"
                }}
              json_output: True
              output: get_subquery_result
            - step_type: "set_variable"
              id: "set_subquery"
              source: "get_subquery_result.pregunta_individual"
              output: "subquery"
            - step_type: "action"
              id: "add_to_memory_type_analysis_global_no_comparative"
              action: "add_to_memory"
              name: "Análisis del tipo de consulta"
              description: |
                Se clasifica la consulta como global o individual.
                Una consulta global (normal o comparativa) requiere obtener información de múltiples reuniones, mientras que una consulta individual se centra en una única reunión.
              result: "Según el análisis, la consulta '{query}' es de tipo global y no comparativa. Por ello, se obtiene una subconsulta para preguntar a cada reunión: {subquery}"
              output: "no_output" # quitar para que no sea obligatorio
            - step_type: "set_variable" #elimnar
              id: "set_subquery2"
              source: "subquery"
              output: "subquery"
          if_false:
            step_type: "if"
            id: "check_comparative_query"
            condition: "is_comparative_query"
            output: "no_output" 
            if_true:
              step_type: "composite"
              id: "check_comparative_query_if_true_composite"
              output: "no_output" # quitar para que no sea obligatorio
              steps:
                - step_type: "action"
                  id: "add_to_memory_type_analysis_global"
                  action: "add_to_memory"
                  name: "Análisis del tipo de consulta"
                  description: |
                    Se clasifica la consulta como global o individual.
                    Una consulta global (normal o comparativa) requiere obtener información de múltiples reuniones, mientras que una consulta individual se centra en una única reunión.
                  result: "Según el análisis, la consulta '{query}' es de tipo global y comparativa. En este caso, no se tranforma en una subconsulta."
                  output: "no_output" # quitar para que no sea obligatorio
                - step_type: "go_to"
                  id: "from_check_global_and_not_comparative_query_to_get_final_response"
                  target_id: "format_documents"
                  output: "no_output"
            if_false:
              step_type: "composite"
              id: "check_comparative_query_if_false_composite"
              output: "no_output" # quitar para que no sea obligatorio
              steps:
                - step_type: "action"
                  id: "add_to_memory_type_analysis_individual"
                  action: "add_to_memory"
                  name: "Análisis del tipo de consulta"
                  description: |
                    Se clasifica la consulta como global o individual.
                    Una consulta global (normal o comparativa) requiere obtener información de múltiples reuniones, mientras que una consulta individual se centra en una única reunión.
                  result: Según el análisis, la consulta '{query}' es de tipo individual. Por ello, no hay necesidad de transformarla en una subconsulta.
                  output: "no_output" # quitar para que no sea obligatorio
                - step_type: "set_variable"
                  id: "set_subquery_for_individual_query"
                  source: "query"
                  output: "subquery"

    - step_type: "for_each"
      id: "generate_responses_by_document"
      iterate_obj: "filtered_documents"
      step:
        step_type: "composite"
        id: "composite_get_response_by_document"
        output: "evidence" # corregir para que no hace falta
        steps:
          - step_type: "action" # igual se puede pasar a donde se creao
            id: "format_document_for_get_response_by_document"
            action: "format_document"
            inputs: ["item"]
            output: document_str
          # - step_type: "action"
          #   action: "evaluate"
          #   step: 
          #     step_type: "llm_call"
          #     id: "get_response_by_document"
          #     json_output: True
          #     prompt: |
          #       Contexto:
          #       {document_str}

          #       Analiza el documento proporcionado y utiliza esa información para responder con precisión a la siguiente pregunta.

          #       Instrucciones:

          #       1. Identifica los datos del documento de donde proviene la información, incluyendo el nombre del archivo y la fecha de la reunión.

          #       2. Redacta una respuesta clara, completa y contextualizada, que:
          #       - Sea directa, bien fundamentada y autónoma.

          #       - Incluya explícitamente el sujeto, la acción y los datos de identificación del documento (como el nombre del archivo, fecha de reunión) dentro del cuerpo de la respuesta, para que quede claro a qué documento se refiere.

          #       - Evite respuestas fragmentadas (por ejemplo, no uses "Sí, llegó", usa "Sí, Ángel llegó el viernes, según el documento [nombre del documento y otros datos de identificacón del documento]").
                
          #       3. Incluye evidencia textual relevante extraída del documento para respaldar tu respuesta. Esta puede ser una cita directa o un resumen claro y preciso.


          #       Formato de salida (JSON):
          #       {{
          #       "nombre_fichero": "(extrae el nombre del fichero analizado)",
          #       "fecha": "(extrae la fecha que aparece en el fichero analizado)",
          #       "pregunta": "(escribe aquí la pregunta original)",
          #       "respuesta": "(respuesta clara, completa, con contexto y referencia explícita al documento)",
          #       "evidencia": "(cita textual o resumen de la evidencia encontrada en el documento)"
          #       }}
          #       Pregunta: {subquery}
          #     output: evidence
          #   json_output: True
          #   prompt: |
          #     Evalúa si la respuesta está respaldada por el contenido del documento proporcionado. Indica si la información mencionada en la respuesta se encuentra explícitamente en el texto o si ha sido inferida o inventada.
          #     Documento: 
          #     {document_str}

          #     Respuesta: {evidence.respuesta}

          #     Responde en formato json con dos claves: "evaluation" y "justification" de tipo string
          #     {{
          #         "evaluation":"Elegir una de las tres opciones: [Explícita/Inferida/Inventada]",
          #         "justification":"Solo si es necesario"
          #     }}
          #   output: evaluation_results

          - step_type: "llm_call"
            id: "get_response_by_document"
            json_output: True
            prompt: |
              Contexto:
              {document_str}

              Analiza el documento proporcionado y utiliza esa información para responder con precisión a la siguiente pregunta.

              Instrucciones:

              1. Identifica los datos del documento de donde proviene la información, incluyendo el nombre del archivo y la fecha de la reunión.

              2. Redacta una respuesta clara, completa y contextualizada, que:
              - Sea directa, bien fundamentada y autónoma.

              - Incluya explícitamente el sujeto, la acción y los datos de identificación del documento (como el nombre del archivo, fecha de reunión) dentro del cuerpo de la respuesta, para que quede claro a qué documento se refiere.

              - Evite respuestas fragmentadas (por ejemplo, no uses "Sí, llegó", usa "Sí, Ángel llegó el viernes, según el documento [nombre del documento y otros datos de identificacón del documento]").
              
              3. Incluye evidencia textual relevante extraída del documento para respaldar tu respuesta. Esta puede ser una cita directa o un resumen claro y preciso.


              Formato de salida (JSON):
              {{
              "nombre_fichero": "(extrae el nombre del fichero analizado)",
              "fecha": "(extrae la fecha que aparece en el fichero analizado)",
              "pregunta": "(escribe aquí la pregunta original)",
              "respuesta": "(respuesta clara, completa, con contexto y referencia explícita al documento)",
              "evidencia": "(cita textual o resumen de la evidencia encontrada en el documento)"
              }}
              Pregunta: {subquery}
            output: evidence
          # - step_type: "llm_call"
          #   id: "evaluate"
          #   json_output: True
          #   prompt: |
          #     Evalúa si la respuesta está respaldada por el contenido del documento proporcionado. Indica si la información mencionada en la respuesta se encuentra explícitamente en el texto o si ha sido inferida o inventada.
          #     Documento: 
          #     {document_str}

          #     Respuesta: {evidence.respuesta}

          #     Responde en formato json con dos claves: "evaluation" y "justification" de tipo string
          #     {{
          #         "evaluation":"Elegir una de las tres opciones: [Explícita/Inferida/Inventada]",
          #         "justification":"Solo si es necesario"
          #     }}
          #   output: evaluation_results
          # - step_type: "if"
          #   id: "check_hallucination"
          #   condition: "evaluation_results.evaluation == 'Inventada'"
          #   output: "evidence" # quitar para que no sea asi
          #   if_true:
          #     step_type: "go_to"
          #     id: "regenerate_response"
          #     target_id: "get_response_by_document"
          #     output: "no_output" # quitar para que no sea obligatorio
          #   if_false:
          #     step_type: "set_variable" # no hace falta si se permitiera modificar contexto directamente.
          #     id: "set_evidence"
          #     source: "evidence"
          #     output: "evidence"
      output: evidences # Falta la parte de evaluación de 5 veces una evidencia
    - step_type: "action"
      id: "format_list"
      action: "format_list"
      inputs: ["evidences"]
      format_template: "- Documento {item.nombre_fichero}({item.fecha}): {item.respuesta} (Evidencia textual: {item.evidencia})\n"
      output: info_str
    - step_type: "go_to"
      id: "go_to_add_to_memory_info_gathering"
      target_id: "add_to_memory_info_gathering"
      output: "no_output" # quitar para que no sea obligatorio
    - step_type: "action"
      id: "format_documents"
      action: "format_documents"
      inputs: ["filtered_documents"]
      output: info_str
    - step_type: "action"
      id: "add_to_memory_info_gathering"
      action: "add_to_memory"
      name: "Recopilación de información"
      description: "Se extrae información relevante de cada documento previamente filtrado mediante la consulta, con el objetivo de obtener los datos necesarios para su posterior análisis."
      result: "Se ha recopilado la siguiente información:\n {info_str}"
      output: "no_output" # quitar para que no sea obligatorio
    - step_type: "action"
      action: "format_memory"
      id: "final_format_memory"
      output: "formatted_steps"
    - step_type: "llm_call"
      id: "get_final_response"
      prompt: |
        Genera una respuesta final directa a la consulta del usuario, utilizando únicamente la información contenida en los pasos que se presentan a continuación. 
        No expliques los pasos ni los filtros aplicados. Limítate a responder a la consulta como si ya hubieras realizado todo el análisis necesario.

        Pasos realizados anteriormente:
        {formatted_steps}

        Consulta del usuario: {query}
      output: final_response



      
    
  